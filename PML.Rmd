---
title: "Practical Machine Learning: Prediction of Manner of Performance in Exercise"
author: "nayehi"
date: "March 23, 2019"
output:
  html_document: default
  pdf_document: default
---

## Summary

The goal of this project is to build a predictive model for how well a test subject performs barbell lifts in a study. This analysis shows it is possible to predict, with more than 99-percent accuracy, a subject's performance of the barbell lift based on a collection of data points from gyroscopes.

## Introduction

This work is undertaken as the final class project in the Practical Machine Learning course. This document is limited to less than 2,000 words and the number of figures is limited to less than 5. A [reader-friendly HTML version](https://nayehi.github.io/PML/PML.html) of this file is available in the GitHub Repository.

**Background Information**

With devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is possible to collect a large amount of data about personal activity. In the study, six participants performed 10 repetitions of the Unilateral Dumbbell Biceps curl in each of the following five ways: exactly according to the specification (Class A); throwing the elbows to the front (Class B); lifting the dumbbell only halfway (Class C); lowering the dumbbell only halfway (Class D); and throwing the hips to the front (Class E). Data was collected by accelerometers on the belts, forearms, and arms of the participants, as well as the dumbells. More information is available in the "Weight Lifting Exercises Dataset" section of the [Human Activity Recognition page](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). The source of this dataset is [Qualitative Activity Recognition of Weight Lifting Exercises](http://web.archive.org/web/20161217164008/http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) by E. Velloso, A. Bulling, H. Gellersen, W. Ugulino, and H. Fuks. 

## Setup

The environment is set up to enable model building, testing and application.

```{r setup, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
set.seed(9999)
```

## Import Data

The data is imported.

```{r importdata}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",header = TRUE)
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",header = TRUE)
```

## Explore Data

The variable to be predicted is "classe," which records which class of exercise was performed in each observation.

The data was explored via dim(training), dim(testing), head(training), head(testing),str(training),str(testing), colnames(training), colnames (testing), summary(training), and summary(testing).

This exploration revealed that the training data consisted of 19622 observations across 160 variables, with the final variable being classe. The testing data consisted of 20 observations across 160 variables, with the final variable being problem_id.

## Clean Data

The inclusion of columns unnecessary for the analysis could substantially increase variability, and so these columns were identified and removed. 

### Identifying Variables for Removal

#### Variables Not Affecting the Model

Columns were identified for removal because the type of information they contained did not affect the classe prediction. 

Column 1 (X) was the row identifier and was not needed. Column 2 (user_name) was not needed for this project. The timestamp columns (3,4,5) also could be removed, since this project did not involve measuring change over time. The window columns (6,7) referred to test specifications not related to performance measures and could be removed.

#### NA/Blanks

The str function revealed there were numerous NA values for many variables. These did not provide sufficient data to advise the model and would be removed. All columns for which NA/blanks accounted for more than 70 percent of the values were identified for removal.

#### Near-Zero Variance

Variables with near-zero variance were identified for removal.

### Removing Variables

```{r remove_unwanted, echo=TRUE}
TrainData1 <- training[,-c(1:7)]
ColRemove <- which(colSums(is.na(TrainData1)|TrainData1=="")>(0.7*dim(TrainData1)[1]))
TrainData2 <- TrainData1[,-ColRemove]
NZV_cols <- nearZeroVar(TrainData2)
if(length(NZV_cols) >0) {TrainData2 <- TrainData2[,-NZV_cols]}
cor_Train <- cor(TrainData2[,-53])
dim(TrainData2)
```

This cleanup resulted in a better data set for achieving the project goal of predicting exercise type performed. The training data (TrainData2) now consisted of the same 19622 observations across 53 variables. Correlations were viewed for the cleaned trainind data; due to length considerations, the results are not displayed.

## Training and Validation Sets

The training data was split into training (60%) and validation (40%) datasets. The validation data would be used to evaluate model fits during tuning; this cross-validation step, along with 5-fold cross-validation applied during model-testing, would be crucial to determine the models' accuracy rates.

```{r makedata}
inTrain <- createDataPartition(TrainData2$classe,p=0.6,list=FALSE)
TrainData <- TrainData2[inTrain,]
ValData <- TrainData2[-inTrain,]
```

Executing dim(TrainData) and dim(ValData) revealed TrainData consisted of 11,776 observations while ValData consisted of 7,746 observations. These were of sufficient size to train and validate models.

## Model - Classification Tree

A Classification Tree model was created with 5-fold cross-validation applied to assess how the model would perform when applied to additional data sets.

```{r classtree}
set.seed(9999)
ctControl <- trainControl(method="cv",number=5)
Mod_CT <- train(classe~.,data=TrainData,method="rpart",trControl=ctControl)
fancyRpartPlot(Mod_CT$finalModel, main="Figure 1: Classification Tree")
```

The Classification Tree model was applied to the Validation Data and a confusion matrix was generated. 

```{r testtree}
trainpred_CT <- predict(Mod_CT,newdata=ValData)
confCT <- confusionMatrix(ValData$classe,trainpred_CT)
confCT$table
confCT$overall[1]
```

The confusion matrix and the overall accuracy displayed above indicated the outcome 'classe' was not well predicted by the other variables using this model.

## Model - Random Forest

A Random Forest model was created with 5-fold cross-validation applied to assess how the model would perform when applied to additional data sets.

```{r rfmod}
TrainData$classe <- as.factor(TrainData$classe)
rfControl <- trainControl(method="cv",number=5)
Mod_RF <- train(classe~., data=TrainData, method="rf", trControl=rfControl,verbose=FALSE)
Mod_RF$finalModel
```

This model showed an accuracy of 99.21 percent and an out-of-sample error rate of only 0.79 percent. An accuracy rate that high could indicate overfitting, despite the application of 5-fold cross-validation. The model was plotted.

```{r plotrf}
plot(Mod_RF,main="Figure 2: Random Forest Model Accuracy by Number of Predictors")
```

This showed the model's accuracy peaked at around 27 predictors, indicating more than half of the predictors were contributing to the model's accuracy. This helped explain the high accuracy rate.

To assess how the Random Forest model performed against a completely different dataset, it was applied to the Validation Data and a confusion matrix was generated. 

```{r predRF}
trainpred_RF <- predict(Mod_RF,newdata=ValData)
confRF <- confusionMatrix(ValData$classe,trainpred_RF)
confRF$table
confRF$overall[1]
```

The confusion matrix was plotted, showing an extremely high accuracy rate for Class A and slightly lower accuracy rates for the other values of classe.

```{r plotconfRF}
plot(confRF$table, col = confRF$byClass,main=paste("Figure 3: Random Forest Confusion Matrix"))
```

The confusion matrix table and the overall accuracy displayed above indicated this model was more than 99 percent accurate when applied to the validation data. Its accuracy was a bit lower against this new data, at 99.08 percent (vice 99.21 for the training data), with an out-of-sample error rate of 0.92 percent, but it remained a highly accurate model. 

##Model - Generalized Boost Regression Model

A Generalized Boost Regression Model was created with 5-fold cross-validation.

```{r GBM}
controlGBM <- trainControl(method="cv",number=5)
Mod_GBM <- train(classe~.,data=TrainData,method="gbm",trControl=controlGBM,verbose=FALSE)
Mod_GBM$finalModel
Mod_GBM
```

The GMB model was applied to the Validation Data and a confusion matrix was generated. 

```{r GBM_val}
trainpred_GBM <- predict(Mod_GBM,newdata=ValData)
confGBM <- confusionMatrix(ValData$classe,trainpred_GBM)
confGBM
```

The GBM Model's overall accuracy was 0.9639, much higher than the Classification Tree but several percentage points lower than the Random Forest Model. 

```{r GBM_plot}
plot(Mod_GBM, main="Figure 4: GBM Model")
```

## Final Model Applied to Test Data

The Random Forest model was superior to the others and was applied to the test set.

Before applying the final model to the test data, the columns in the test data were set so they matched the columns in the Training data set.

```{r cleantest}
common_cols <- intersect(colnames(testing), colnames(TrainData))
TestData <- testing[, common_cols]
testpred <- predict(Mod_RF,newdata=TestData)
testpred
```

## Conclusion

The Random Forest Model is superior to the other models for this data in terms of sensitivity, specificity, and positive and negative predictive values. Its out-of-sample rates are 99.08 percent for accuracy and 0.92 percent for error, making this an extremely accurate model.
